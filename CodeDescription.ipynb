{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration flags for file inclusion\n",
    "include_config = {\n",
    "    'main_code': False,           # MediaCoverageAnalysis.py\n",
    "    'helpers': True,              # Utils/Helpers.py  - Enabled to extract helper functions\n",
    "    'outputs': True,              # Utils/Outputs.py  - Enabled to extract output functions\n",
    "    'chatbots': False,            # Classes/SimplifiedChatbots.py\n",
    "    'doc_processor': False        # Classes/DocumentProcessor.py\n",
    "}\n",
    "\n",
    "# Output functions selection (set to True for the function you want to analyze)\n",
    "output_functions_config = {\n",
    "    'generate_journalist_list_output': False,     # Generate list of journalists and media outlets\n",
    "    'generate_insights_output': False,            # Generate article insights\n",
    "    'generate_issue_analysis_output': False,      # Generate issue analysis\n",
    "    'generate_topics_output': False,              # Generate topic summaries\n",
    "    'generate_analytics_output': False,            # Generate media analytics\n",
    "    'generate_stakeholder_quotes': False,         # Generate stakeholder analysis\n",
    "    'generate_consolidated_stakeholder_analysis': True,  # Generate consolidated stakeholder analysis\n",
    "    'generate_journalist_article_list': False,    # Generate article list for a journalist\n",
    "    'generate_journalist_profile': False,         # Generate journalist profile\n",
    "    'analyze_journalist_topic_coverage': False,   # Generate journalist topic coverage analysis\n",
    "    'generate_journalist_analysis_output': False  # Generate journalist analytics\n",
    "}\n",
    "\n",
    "# File paths (modify these according to your project structure)\n",
    "file_paths = {\n",
    "    'main_code': 'MediaCoverageAnalysis.py',\n",
    "    'helpers': 'Utils/Helpers.py',\n",
    "    'outputs': 'Utils/Outputs.py',\n",
    "    'chatbots': 'Classes/SimplifiedChatbots.py',\n",
    "    'doc_processor': 'Classes/DocumentProcessor.py'\n",
    "}\n",
    "\n",
    "def read_file_content(file_path):\n",
    "    \"\"\"Read and return the content of a Python file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading {file_path}: {str(e)}\"\n",
    "\n",
    "def count_lines_of_code(content):\n",
    "    \"\"\"Count non-empty, non-comment lines of code.\"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    count = 0\n",
    "    in_multiline_comment = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:  # Skip empty lines\n",
    "            continue\n",
    "            \n",
    "        # Handle multiline comments\n",
    "        if '\"\"\"' in line or \"'''\" in line:\n",
    "            if in_multiline_comment:\n",
    "                in_multiline_comment = False\n",
    "                continue\n",
    "            elif line.count('\"\"\"') == 1 or line.count(\"'''\") == 1:\n",
    "                in_multiline_comment = True\n",
    "                continue\n",
    "                \n",
    "        if in_multiline_comment:\n",
    "            continue\n",
    "            \n",
    "        # Skip single-line comments\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "            \n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def extract_imports(content):\n",
    "    \"\"\"\n",
    "    Extract import statements from the code with special handling for wildcard imports.\n",
    "    Identifies potential helper function imports.\n",
    "    \"\"\"\n",
    "    import ast\n",
    "    import re\n",
    "    \n",
    "    imports = []\n",
    "    helpers_wildcard_import = False\n",
    "    specific_helper_imports = []\n",
    "    \n",
    "    # First, check for wildcard imports from Helpers using regex\n",
    "    wildcard_match = re.search(r'from\\s+Utils\\.Helpers\\s+import\\s+\\*', content)\n",
    "    if wildcard_match:\n",
    "        helpers_wildcard_import = True\n",
    "        imports.append(\"Utils.Helpers.*\")\n",
    "    \n",
    "    # Now use AST to get specific imports\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.Import):\n",
    "                for name in node.names:\n",
    "                    imports.append(name.name)\n",
    "            elif isinstance(node, ast.ImportFrom):\n",
    "                module = node.module or ''\n",
    "                for name in node.names:\n",
    "                    imported_name = f\"{module}.{name.name}\"\n",
    "                    imports.append(imported_name)\n",
    "                    \n",
    "                    # Track specific helper imports\n",
    "                    if module == 'Utils.Helpers':\n",
    "                        if name.name != '*':  # Skip wildcards as we handle them separately\n",
    "                            specific_helper_imports.append(name.name)\n",
    "                            \n",
    "        # Add a special marker for helpers analysis\n",
    "        if helpers_wildcard_import or specific_helper_imports:\n",
    "            if helpers_wildcard_import:\n",
    "                imports.append(\"__ALL_HELPERS_AVAILABLE__\")\n",
    "            else:\n",
    "                imports.append(f\"__SPECIFIC_HELPERS__: {', '.join(specific_helper_imports)}\")\n",
    "                \n",
    "        return sorted(set(imports))\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting imports: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_docstring(content):\n",
    "    \"\"\"Extract the module-level docstring from Python code.\"\"\"\n",
    "    import ast\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        if ast.get_docstring(tree):\n",
    "            return ast.get_docstring(tree)\n",
    "        return \"⚠️ No module-level docstring found. Consider adding a description of this module's purpose.\"\n",
    "    except:\n",
    "        return \"⚠️ Unable to parse code for docstring.\"\n",
    "\n",
    "def extract_function_calls(function_code):\n",
    "    \"\"\"Extract function calls from a function's code.\"\"\"\n",
    "    import ast\n",
    "    import re\n",
    "    \n",
    "    function_calls = set()\n",
    "    \n",
    "    # Extract function calls using AST\n",
    "    try:\n",
    "        tree = ast.parse(function_code)\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.Call):\n",
    "                if isinstance(node.func, ast.Name):\n",
    "                    # Direct function call like function_name()\n",
    "                    function_calls.add(node.func.id)\n",
    "                elif isinstance(node.func, ast.Attribute):\n",
    "                    # Method call like object.method()\n",
    "                    # We're only interested in potential helpers, not methods\n",
    "                    if isinstance(node.func.value, ast.Name) and node.func.value.id == 'self':\n",
    "                        function_calls.add(node.func.attr)\n",
    "    except Exception as e:\n",
    "        print(\"\")\n",
    "    \n",
    "    # Use regex as a fallback to find potential function calls\n",
    "    # This regex looks for words followed by parentheses\n",
    "    potential_calls = re.findall(r'(\\w+)\\s*\\(', function_code)\n",
    "    function_calls.update(potential_calls)\n",
    "    \n",
    "    # Advanced regex to find imported helper functions being called\n",
    "    # Look for patterns like: from Utils.Helpers import *\n",
    "    import_matches = re.findall(r'from\\s+Utils\\.Helpers\\s+import\\s+\\*', function_code)\n",
    "    if import_matches:\n",
    "        # If we have \"from Utils.Helpers import *\", we need to be more careful\n",
    "        # as any function call could potentially be a helper\n",
    "        pass\n",
    "        \n",
    "    # Filter out Python built-ins and common methods\n",
    "    builtins = ['print', 'len', 'str', 'int', 'float', 'list', 'dict', 'set', \n",
    "                'tuple', 'sum', 'min', 'max', 'sorted', 'range', 'enumerate',\n",
    "                'zip', 'map', 'filter', 'round', 'open', 'os', 'logging', 'read',\n",
    "                'write', 'append', 'pop', 'sort', 'datetime', 'strptime', 'Path',\n",
    "                'mkdir', 'join', 'isfile', 'exists', 'dirname', 'lower', 'upper',\n",
    "                'strip', 'replace', 'split', 'startswith', 'endswith', 'format',\n",
    "                'get', 'keys', 'values', 'items', 'update', 'isinstance', 'raise']\n",
    "    function_calls = {call for call in function_calls if call not in builtins and not call.startswith('__')}\n",
    "    \n",
    "    return function_calls\n",
    "\n",
    "def extract_functions(content, selected_functions=None):\n",
    "    \"\"\"\n",
    "    Extract function names, arguments, their docstrings, and source code from Python code.\n",
    "    If selected_functions is provided, only extract those specific functions.\n",
    "    \"\"\"\n",
    "    import ast\n",
    "    import re\n",
    "    \n",
    "    # First, scan for all helper modules imported\n",
    "    helper_imports = []\n",
    "    helper_wildcard = False\n",
    "    import_lines = re.findall(r'^from\\s+Utils\\.Helpers\\s+import\\s+(.+)$', content, re.MULTILINE)\n",
    "    for line in import_lines:\n",
    "        if '*' in line:\n",
    "            helper_wildcard = True\n",
    "        else:\n",
    "            helpers = [h.strip() for h in line.split(',')]\n",
    "            helper_imports.extend(helpers)\n",
    "    \n",
    "    print(f\"Helper imports found: {helper_imports}, Wildcard: {helper_wildcard}\")\n",
    "    \n",
    "    functions = []\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                # Skip if we're only looking for specific functions\n",
    "                if selected_functions and node.name not in selected_functions:\n",
    "                    continue\n",
    "                    \n",
    "                # Get function arguments\n",
    "                args = []\n",
    "                for arg in node.args.args:\n",
    "                    args.append(arg.arg)\n",
    "                \n",
    "                # Get docstring\n",
    "                doc = ast.get_docstring(node) or \"⚠️ No docstring provided\"\n",
    "                \n",
    "                # Get function source code\n",
    "                source_lines = content.split('\\n')\n",
    "                start_line = node.lineno - 1\n",
    "                end_line = node.end_lineno\n",
    "                source_code = '\\n'.join(source_lines[start_line:end_line])\n",
    "                \n",
    "                # Extract function calls within this function\n",
    "                function_calls = extract_function_calls(source_code)\n",
    "                \n",
    "                # If we're analyzing an output function and we have helper imports,\n",
    "                # make sure to include those helper functions in function_calls\n",
    "                if not selected_functions or node.name in selected_functions:\n",
    "                    if helper_wildcard:\n",
    "                        # Just note that this function might call any helper\n",
    "                        pass\n",
    "                    else:\n",
    "                        # Add specifically imported helper functions if they appear in the code\n",
    "                        for helper in helper_imports:\n",
    "                            pattern = r'\\b' + re.escape(helper) + r'\\s*\\('\n",
    "                            if re.search(pattern, source_code):\n",
    "                                function_calls.add(helper)\n",
    "                                print(f\"Added explicitly imported helper: {helper} to {node.name}\")\n",
    "                \n",
    "                functions.append({\n",
    "                    'name': node.name,\n",
    "                    'args': args,\n",
    "                    'docstring': doc,\n",
    "                    'source_code': source_code,\n",
    "                    'function_calls': function_calls,\n",
    "                    'imports_all_helpers': helper_wildcard,\n",
    "                    'specific_helper_imports': helper_imports\n",
    "                })\n",
    "        return functions\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting functions: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_classes(content, extract_all_methods=False, selected_methods=None):\n",
    "    \"\"\"Extract class names, methods, their docstrings, and source code from Python code.\"\"\"\n",
    "    import ast\n",
    "    classes = []\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.ClassDef):\n",
    "                methods = []\n",
    "                for child in node.body:\n",
    "                    if isinstance(child, ast.FunctionDef):\n",
    "                        # Skip if we're looking for specific methods and this isn't one\n",
    "                        if not extract_all_methods and selected_methods and child.name not in selected_methods:\n",
    "                            continue\n",
    "                            \n",
    "                        args = [arg.arg for arg in child.args.args]\n",
    "                        doc = ast.get_docstring(child) or \"⚠️ No docstring provided\"\n",
    "                        \n",
    "                        # Get method source code\n",
    "                        source_lines = content.split('\\n')\n",
    "                        start_line = child.lineno - 1\n",
    "                        end_line = child.end_lineno\n",
    "                        source_code = '\\n'.join(source_lines[start_line:end_line])\n",
    "                        \n",
    "                        # Extract function calls within this method\n",
    "                        function_calls = extract_function_calls(source_code)\n",
    "                        \n",
    "                        methods.append({\n",
    "                            'name': child.name,\n",
    "                            'args': args,\n",
    "                            'docstring': doc,\n",
    "                            'source_code': source_code,\n",
    "                            'function_calls': function_calls\n",
    "                        })\n",
    "                \n",
    "                # Get class source code\n",
    "                source_lines = content.split('\\n')\n",
    "                start_line = node.lineno - 1\n",
    "                end_line = node.end_lineno\n",
    "                source_code = '\\n'.join(source_lines[start_line:end_line])\n",
    "                \n",
    "                doc = ast.get_docstring(node) or \"⚠️ No docstring provided\"\n",
    "                classes.append({\n",
    "                    'name': node.name,\n",
    "                    'docstring': doc,\n",
    "                    'methods': methods,\n",
    "                    'source_code': source_code\n",
    "                })\n",
    "        return classes\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting classes: {e}\")\n",
    "        return []\n",
    "\n",
    "def find_dependent_functions(output_functions, helper_functions):\n",
    "    \"\"\"\n",
    "    Find all helper functions that are called by the selected output functions,\n",
    "    including indirect dependencies (recursively).\n",
    "    \"\"\"\n",
    "    # Start with direct dependencies\n",
    "    dependencies = set()\n",
    "    functions_to_check = set()\n",
    "    \n",
    "    # Collect initial function calls from output functions\n",
    "    for func in output_functions:\n",
    "        functions_to_check.update(func['function_calls'])\n",
    "        \n",
    "    print(f\"Initial functions to check from output functions: {functions_to_check}\")\n",
    "    \n",
    "    # Keep track of functions we've already processed to avoid infinite recursion\n",
    "    processed_functions = set()\n",
    "    \n",
    "    # Process functions until no more dependencies are found\n",
    "    while functions_to_check:\n",
    "        current_func_name = functions_to_check.pop()\n",
    "        \n",
    "        if current_func_name in processed_functions:\n",
    "            continue\n",
    "            \n",
    "        processed_functions.add(current_func_name)\n",
    "        \n",
    "        # Look for this function in helper functions\n",
    "        found = False\n",
    "        for helper in helper_functions:\n",
    "            if helper['name'] == current_func_name:\n",
    "                dependencies.add(current_func_name)\n",
    "                # Add this helper's dependencies to check\n",
    "                new_dependencies = helper['function_calls'] - processed_functions\n",
    "                functions_to_check.update(new_dependencies)\n",
    "                found = True\n",
    "                print(f\"Found helper: {current_func_name}, adding dependencies: {new_dependencies}\")\n",
    "                break\n",
    "\n",
    "    # For debugging: print the final dependencies\n",
    "    print(f\"Final dependencies: {dependencies}\")\n",
    "    \n",
    "    return dependencies\n",
    "\n",
    "def generate_code_description():\n",
    "    \"\"\"Generate a structured description of the codebase focusing on selected output functions.\"\"\"\n",
    "    import re\n",
    "    description = []\n",
    "    description.append(\"# Code Structure and Documentation with Focus on Output Functions\\n\")\n",
    "    \n",
    "    # First, read all file contents\n",
    "    file_contents = {}\n",
    "    for key, path in file_paths.items():\n",
    "        if include_config.get(key, False):\n",
    "            file_contents[key] = read_file_content(path)\n",
    "    \n",
    "    # Step 1: Extract outputs first to identify selected functions\n",
    "    if include_config.get('outputs', False) and 'outputs' in file_contents:\n",
    "        outputs_content = file_contents['outputs']\n",
    "        output_functions = extract_functions(outputs_content)\n",
    "        \n",
    "        # Get imports from outputs file to better understand dependencies\n",
    "        output_imports = extract_imports(outputs_content)\n",
    "        uses_helper_wildcard = any(\"Utils.Helpers.*\" in imp for imp in output_imports) or any(\"__ALL_HELPERS_AVAILABLE__\" in imp for imp in output_imports)\n",
    "        \n",
    "        print(f\"Imports in Outputs.py: {output_imports}\")\n",
    "        print(f\"Uses Helper wildcard import: {uses_helper_wildcard}\")\n",
    "        \n",
    "        # Filter to only selected output functions\n",
    "        selected_output_functions = []\n",
    "        for func in output_functions:\n",
    "            if output_functions_config.get(func['name'], False):\n",
    "                selected_output_functions.append(func)\n",
    "        \n",
    "        # If no output functions were explicitly selected, use all of them  \n",
    "        if not any(output_functions_config.values()):\n",
    "            selected_output_functions = output_functions\n",
    "            \n",
    "        print(f\"Selected output functions: {[func['name'] for func in selected_output_functions]}\")\n",
    "    else:\n",
    "        selected_output_functions = []\n",
    "    \n",
    "    # Step 2: Extract helper functions\n",
    "    if include_config.get('helpers', False) and 'helpers' in file_contents:\n",
    "        helpers_content = file_contents['helpers']\n",
    "        helper_functions = extract_functions(helpers_content)\n",
    "        print(f\"Available helper functions: {[func['name'] for func in helper_functions]}\")\n",
    "    else:\n",
    "        helper_functions = []\n",
    "    \n",
    "    # Step 3: Find dependent helper functions\n",
    "    if selected_output_functions and helper_functions:\n",
    "        # If outputs.py imports all helpers with *, we need to do a deeper scan\n",
    "        if uses_helper_wildcard:\n",
    "            print(\"Detected wildcard import of Helpers, performing deeper dependency analysis\")\n",
    "            # First find direct dependencies\n",
    "            dependent_function_names = find_dependent_functions(selected_output_functions, helper_functions)\n",
    "            \n",
    "            # Search for function calls in the output functions that match helper function names\n",
    "            for output_func in selected_output_functions:\n",
    "                source_code = output_func['source_code']\n",
    "                for helper_func in helper_functions:\n",
    "                    # Check if helper function name appears in source code\n",
    "                    if re.search(r'\\b' + re.escape(helper_func['name']) + r'\\s*\\(', source_code):\n",
    "                        dependent_function_names.add(helper_func['name'])\n",
    "                        print(f\"Found additional helper function call: {helper_func['name']} in {output_func['name']}\")\n",
    "        else:\n",
    "            # Standard dependency resolution\n",
    "            dependent_function_names = find_dependent_functions(selected_output_functions, helper_functions)\n",
    "        \n",
    "        dependent_helper_functions = [f for f in helper_functions if f['name'] in dependent_function_names]\n",
    "        print(f\"Found {len(dependent_helper_functions)} dependent helper functions\")\n",
    "    else:\n",
    "        dependent_helper_functions = []\n",
    "    \n",
    "    # Generate output for selected output functions\n",
    "    if selected_output_functions:\n",
    "        description.append(\"\\n## Selected Output Functions from Utils/Outputs.py\\n\")\n",
    "        \n",
    "        # Add file statistics\n",
    "        outputs_content = file_contents.get('outputs', '')\n",
    "        loc = count_lines_of_code(outputs_content)\n",
    "        description.append(f\"\\n### File Statistics\")\n",
    "        description.append(f\"- Lines of code: {loc}\")\n",
    "        description.append(f\"- Selected output functions: {len(selected_output_functions)}\")\n",
    "        \n",
    "        # Add imports\n",
    "        imports = extract_imports(outputs_content)\n",
    "        if imports:\n",
    "            description.append(\"\\n### Dependencies\")\n",
    "            description.append(\"The following modules are imported:\")\n",
    "            for imp in imports:\n",
    "                description.append(f\"- `{imp}`\")\n",
    "        \n",
    "        # Add selected output functions\n",
    "        description.append(\"\\n### Selected Output Functions\")\n",
    "        for func in selected_output_functions:\n",
    "            args_str = ', '.join(func['args'])\n",
    "            description.append(f\"\\n#### `{func['name']}({args_str})`\")\n",
    "            description.append(func['docstring'])\n",
    "            description.append(\"\\nFunction Implementation:\")\n",
    "            description.append(\"```python\")\n",
    "            description.append(func['source_code'])\n",
    "            description.append(\"```\")\n",
    "            \n",
    "            if func['function_calls']:\n",
    "                description.append(\"\\nCalls the following functions:\")\n",
    "                for call in sorted(func['function_calls']):\n",
    "                    description.append(f\"- `{call}()`\")\n",
    "    \n",
    "    # Generate output for dependent helper functions\n",
    "    if dependent_helper_functions:\n",
    "        description.append(\"\\n## Dependent Helper Functions from Utils/Helpers.py\\n\")\n",
    "        \n",
    "        helpers_content = file_contents.get('helpers', '')\n",
    "        loc = count_lines_of_code(helpers_content)\n",
    "        description.append(f\"\\n### File Statistics\")\n",
    "        description.append(f\"- Lines of code: {loc}\")\n",
    "        description.append(f\"- Dependent helper functions: {len(dependent_helper_functions)}\")\n",
    "        \n",
    "        # Add imports\n",
    "        imports = extract_imports(helpers_content)\n",
    "        if imports:\n",
    "            description.append(\"\\n### Dependencies\")\n",
    "            description.append(\"The following modules are imported:\")\n",
    "            for imp in imports:\n",
    "                description.append(f\"- `{imp}`\")\n",
    "        \n",
    "        # Add dependent helper functions\n",
    "        description.append(\"\\n### Helper Functions\")\n",
    "        for func in dependent_helper_functions:\n",
    "            args_str = ', '.join(func['args'])\n",
    "            description.append(f\"\\n#### `{func['name']}({args_str})`\")\n",
    "            description.append(func['docstring'])\n",
    "            description.append(\"\\nFunction Implementation:\")\n",
    "            description.append(\"```python\")\n",
    "            description.append(func['source_code'])\n",
    "            description.append(\"```\")\n",
    "            \n",
    "            if func['function_calls']:\n",
    "                description.append(\"\\nCalls the following functions:\")\n",
    "                for call in sorted(func['function_calls']):\n",
    "                    description.append(f\"- `{call}()`\")\n",
    "    \n",
    "    # Add other files if requested\n",
    "    for key in ['main_code', 'chatbots', 'doc_processor']:\n",
    "        if not include_config.get(key, False):\n",
    "            continue\n",
    "            \n",
    "        if key not in file_contents:\n",
    "            continue\n",
    "            \n",
    "        content = file_contents[key]\n",
    "        file_path = file_paths[key]\n",
    "        \n",
    "        description.append(f\"\\n## {file_path}\\n\")\n",
    "        \n",
    "        # Add code statistics\n",
    "        loc = count_lines_of_code(content)\n",
    "        description.append(f\"\\n### File Statistics\")\n",
    "        description.append(f\"- Lines of code: {loc}\")\n",
    "        \n",
    "        # Add imports\n",
    "        imports = extract_imports(content)\n",
    "        if imports:\n",
    "            description.append(\"\\n### Dependencies\")\n",
    "            description.append(\"The following modules are imported:\")\n",
    "            for imp in imports:\n",
    "                description.append(f\"- `{imp}`\")\n",
    "        \n",
    "        # Add module docstring\n",
    "        module_doc = extract_docstring(content)\n",
    "        description.append(\"\\n### Module Description\")\n",
    "        description.append(module_doc)\n",
    "        \n",
    "        # Add full source code\n",
    "        description.append(\"\\n### Source Code\")\n",
    "        description.append(\"```python\")\n",
    "        description.append(content)\n",
    "        description.append(\"```\")\n",
    "    \n",
    "    return '\\n'.join(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper imports found: [], Wildcard: False\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Available helper functions: ['extract_hyperlinks', 'add_links_to_articles', 'check_input_paths', 'parse_relative_date', 'clean_date_string', 'process_article_date', 'ensure_directory_exists', 'process_pdfs', 'get_files', 'save_data_to_json', 'load_data_from_json', 'extract_metadata', 'clean_articles', 'filter_duplicates', 'get_embeddings', 'filter_relevant_articles', 'filter_top_categories', 'extract_sentiment_score', 'save_plot_base64', 'create_bar_chart_compiled_insights', 'create_sentiment_graph', 'create_category_sentiment_graph', 'create_horizontal_bar_chart', 'create_stacked_bar_chart', 'generate_media_outlet_pie_chart', 'generate_media_outlet_tone_chart', 'generate_overall_sentiment_trend', 'generate_sentiment_trends_by_category', 'generate_articles_per_category', 'generate_category_tone_chart', 'generate_top_journalists_chart', 'read_insights_content', 'generate_toc', 'extract_categories', 'generate_markdown_report', 'process_stakeholder_info', 'process_markdown_table', 'translate_content', 'convert_md_to_pdf', 'create_markdown_anchor', 'find_extrema_points', 'analyze_sentiment_period', 'generate_sentiment_analysis_section', 'find_category_extrema', 'analyze_category_period', 'generate_category_sentiment_section', 'setup_journalist_directories', 'preprocess_journalist_articles', 'generate_introduction', 'preprocess_articles', 'setup_logging', 'log_function_call', 'parse_stakeholder_table', 'process_markdown_file', 'get_coverage_categories', 'create_custom_colormap', 'get_text_color', 'create_professional_pie', 'create_multiple_pie_charts', 'determine_main_categories', 'classify_single_article', 'classify_articles', 'generate_categorization_markdown', 'json_serial', 'save_to_json', 'extract_organizations', 'extract_people', 'extract_entities', 'analyze_entity_sentiment', 'analyze_article_sentiments', 'analyze_all_sentiments', 'update_markdown_with_sentiments', 'generate_top_journalists_analysis', 'parse_ranking_output', 'generate_markdown_table', 'generate_publication_timeline_chart', 'generate_publication_timeline_section', 'text_similarity', 'needs_entity_extraction', 'needs_sentiment_analysis', 'get_stakeholder', 'get_quote', 'quote_similarity', 'wrapper']\n",
      "# Code Structure and Documentation with Focus on Output Functions\n",
      "\n",
      "\n",
      "## MediaCoverageAnalysis.py\n",
      "\n",
      "\n",
      "### File Statistics\n",
      "- Lines of code: 443\n",
      "\n",
      "### Dependencies\n",
      "The following modules are imported:\n",
      "- `Classes.DocumentProcessor.CitationProcessor`\n",
      "- `Classes.DocumentProcessor.DocumentProcessor`\n",
      "- `Classes.ProgramSummaryTracker.ProgramSummaryTracker`\n",
      "- `Utils.Helpers.*`\n",
      "- `Utils.Outputs.*`\n",
      "- `Utils.Outputs.generate_journalist_article_list`\n",
      "- `__ALL_HELPERS_AVAILABLE__`\n",
      "- `argparse`\n",
      "- `gradio`\n",
      "- `json`\n",
      "- `logging`\n",
      "- `traceback`\n",
      "\n",
      "### Module Description\n",
      "⚠️ No module-level docstring found. Consider adding a description of this module's purpose.\n",
      "\n",
      "### Source Code\n",
      "```python\n",
      "import argparse\n",
      "import gradio as gr\n",
      "from Classes.DocumentProcessor import DocumentProcessor, CitationProcessor\n",
      "from Classes.ProgramSummaryTracker import ProgramSummaryTracker\n",
      "from Utils.Helpers import *\n",
      "from Utils.Outputs import *\n",
      "import logging\n",
      "import traceback\n",
      "import logging\n",
      "import json\n",
      "\n",
      "logging.basicConfig(level=logging.DEBUG, filename='app.log', filemode='w',\n",
      "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
      "\n",
      "def gradio_journalist_list(company_name, file_folder, docx_file_path, industry_of_interest, \n",
      "                           region, journalist_name, language, force_reprocess,\n",
      "                           generate_journalist_list, generate_insights, \n",
      "                           generate_analysis, generate_topics, \n",
      "                           generate_analytics, generate_quotes,\n",
      "                           generate_consolidated_quotes, generate_journalist_article_list, \n",
      "                           generate_journalist_coverage_profile,\n",
      "                           generate_topic_analysis, generate_journalist_analysis,  # NEW parameter\n",
      "                           topic_focus=None):\n",
      "    try:\n",
      "        # Set up base folder and logging (unchanged)\n",
      "        if file_folder:\n",
      "            base_folder = os.path.dirname(os.path.dirname(file_folder))\n",
      "        elif docx_file_path:\n",
      "            base_folder = os.path.dirname(os.path.dirname(docx_file_path))\n",
      "        else:\n",
      "            base_folder = os.getcwd()\n",
      "        \n",
      "        setup_logging(base_folder)\n",
      "\n",
      "        # Initialize the program summary tracker\n",
      "        tracker = ProgramSummaryTracker(output_dir=base_folder)\n",
      "\n",
      "        # Validate input paths, process documents, etc.\n",
      "        pdf_folder = file_folder.strip() if file_folder else None\n",
      "        docx_path = docx_file_path.strip() if docx_file_path else None\n",
      "        \n",
      "        if not check_input_paths(pdf_folder, docx_path):\n",
      "            error_msg = \"No valid input paths provided. Please check your folder/file paths.\"\n",
      "            return [error_msg] * 11  # Now 11 outputs\n",
      "\n",
      "        processor = DocumentProcessor(\n",
      "            min_length=1000,\n",
      "            max_length=25500,\n",
      "            similarity_threshold=0.8\n",
      "        )\n",
      "\n",
      "        articles = processor.process_documents(\n",
      "            pdf_folder_path=file_folder if file_folder.strip() else None,\n",
      "            docx_file_path=docx_file_path if docx_file_path.strip() else None,\n",
      "            docx_separator=\"--\"\n",
      "        )\n",
      "\n",
      "        if not articles:\n",
      "            error_msg = \"No valid articles found after processing documents\"\n",
      "            return [error_msg] * 11\n",
      "\n",
      "        # Determine if this is a journalist analysis (based on provided journalist_name and options)\n",
      "        is_journalist_analysis = bool(journalist_name and (generate_journalist_coverage_profile or generate_topic_analysis or generate_journalist_article_list or generate_journalist_analysis))\n",
      "\n",
      "        if is_journalist_analysis:\n",
      "            logging.info(f\"Starting journalist analysis for {journalist_name}\")\n",
      "            articles_sorted, general_folder, success = preprocess_journalist_articles(\n",
      "                journalist_name=journalist_name,\n",
      "                articles=articles,\n",
      "                news_folder_path=file_folder,\n",
      "                force_reprocess=force_reprocess\n",
      "            )\n",
      "            if not success:\n",
      "                error_msg = \"Failed to preprocess journalist articles\"\n",
      "                return [error_msg] * 11\n",
      "\n",
      "            # For journalist analysis mode, we keep some default placeholder outputs for the media analysis slots.\n",
      "            results = [\"Journalist analysis mode\"] * 7  # first 7 outputs (placeholders)\n",
      "\n",
      "            journalist_profile_result = \"Journalist profile not requested\"\n",
      "            topic_analysis_result = \"Topic analysis not requested\"\n",
      "            article_list_result = \"Articles list not requested\"\n",
      "            journalist_analysis_result = \"Journalist analysis not requested\"  # NEW output\n",
      "\n",
      "            # Generate article list if requested\n",
      "            if generate_journalist_article_list:\n",
      "                try:\n",
      "                    logging.info(\"Generating journalist article list\")\n",
      "                    from Utils.Outputs import generate_journalist_article_list\n",
      "                    article_list_result = generate_journalist_article_list(\n",
      "                        articles_sorted=articles_sorted,\n",
      "                        journalist_name=journalist_name,\n",
      "                        general_folder=general_folder,\n",
      "                        language=language\n",
      "                    )\n",
      "                except Exception as e:\n",
      "                    logging.error(f\"Error generating journalist article list: {str(e)}\")\n",
      "                    article_list_result = f\"Error generating article list: {str(e)}\"\n",
      "\n",
      "            # Generate journalist profile if requested\n",
      "            if generate_journalist_coverage_profile:\n",
      "                try:\n",
      "                    logging.info(\"Generating journalist profile\")\n",
      "                    journalist_profile_result = generate_journalist_profile(\n",
      "                        articles_sorted=articles_sorted,\n",
      "                        journalist_name=journalist_name,\n",
      "                        news_folder_path=file_folder,\n",
      "                        language=language\n",
      "                    )\n",
      "                except Exception as e:\n",
      "                    logging.error(f\"Error generating journalist profile: {str(e)}\")\n",
      "                    journalist_profile_result = f\"Error generating journalist profile: {str(e)}\"\n",
      "\n",
      "            # Generate topic analysis if requested\n",
      "            if generate_topic_analysis and topic_focus and topic_focus.strip():\n",
      "                try:\n",
      "                    logging.info(f\"Generating topic analysis for {topic_focus}\")\n",
      "                    topic_analysis_result = analyze_journalist_topic_coverage(\n",
      "                        articles_sorted=articles_sorted,\n",
      "                        journalist_name=journalist_name,\n",
      "                        topic_focus=topic_focus,\n",
      "                        general_folder=general_folder,\n",
      "                        language=language\n",
      "                    )\n",
      "                except Exception as e:\n",
      "                    logging.error(f\"Error in topic analysis: {str(e)}\")\n",
      "                    topic_analysis_result = f\"Error generating topic analysis: {str(e)}\"\n",
      "\n",
      "            # NEW: Generate comprehensive journalist analysis if requested\n",
      "            if generate_journalist_analysis:\n",
      "                try:\n",
      "                    logging.info(\"Generating comprehensive journalist analysis\")\n",
      "                    journalist_analysis_result = generate_journalist_analysis_output(\n",
      "                        articles_sorted=articles_sorted,\n",
      "                        journalist_name=journalist_name,\n",
      "                        general_folder=general_folder,\n",
      "                        language=language\n",
      "                    )\n",
      "                except Exception as e:\n",
      "                    logging.error(f\"Error generating journalist analysis: {str(e)}\")\n",
      "                    journalist_analysis_result = f\"Error generating journalist analysis: {str(e)}\"\n",
      "\n",
      "            # Return all 11 outputs for journalist analysis:\n",
      "            # (The first 7 outputs are placeholders for media analysis outputs not used in journalist analysis mode.)\n",
      "            return results + [journalist_profile_result, topic_analysis_result, article_list_result, journalist_analysis_result]\n",
      "\n",
      "        else:\n",
      "            # Company analysis branch\n",
      "            logging.info(\"Starting company analysis\")\n",
      "            try:\n",
      "                articles_sorted, general_folder, directories_created = preprocess_articles(\n",
      "                    company_name=company_name,\n",
      "                    articles=articles,\n",
      "                    industry_of_interest=industry_of_interest,\n",
      "                    region=region\n",
      "                )\n",
      "\n",
      "                if not directories_created:\n",
      "                    error_msg = \"Failed to create necessary directories\"\n",
      "                    return [error_msg] * 11\n",
      "\n",
      "                md_content = \"Articles' reference list not requested\"\n",
      "                summary_insights = \"Summary insights not requested\"\n",
      "                combined_analysis = \"Comprehensive issues analysis not requested\"\n",
      "                topics_summaries = \"Topic summaries not requested\"\n",
      "                media_analytics = \"Media analytics not requested\"\n",
      "                stakeholder_quotes = \"Stakeholder quotes analysis not requested\"\n",
      "                consolidated_quotes = \"Consolidated stakeholder analysis not requested\"\n",
      "                journalist_profile = \"Journalist profile not requested\"\n",
      "                topic_analysis_result = \"Topic analysis not requested\"\n",
      "                journalist_articles_list = \"Articles list not requested\"\n",
      "                # Since this is not journalist analysis, the new output is not applicable.\n",
      "                journalist_analysis_result = \"Journalist analytics not requested\"\n",
      "\n",
      "                if generate_journalist_list:\n",
      "                    try:\n",
      "                        logging.info(\"Generating Articles' reference list\")\n",
      "                        md_content = generate_journalist_list_output(\n",
      "                            articles_sorted=articles_sorted,\n",
      "                            company_name=company_name,\n",
      "                            general_folder=general_folder,\n",
      "                            language=language\n",
      "                        )\n",
      "                    except Exception as e:\n",
      "                        logging.error(f\"Error generating articles list: {str(e)}\")\n",
      "                        md_content = f\"Error generating articles list: {str(e)}\"\n",
      "\n",
      "                if generate_insights:\n",
      "                    try:\n",
      "                        logging.info(\"Generating insights\")\n",
      "                        summary_insights = generate_insights_output(\n",
      "                            articles_sorted=articles_sorted,\n",
      "                            company_name=company_name,\n",
      "                            general_folder=general_folder,\n",
      "                            industry_of_interest=industry_of_interest,\n",
      "                            region=region,\n",
      "                            language=language\n",
      "                        )\n",
      "                    except Exception as e:\n",
      "                        logging.error(f\"Error generating insights: {str(e)}\")\n",
      "                        summary_insights = f\"Error generating insights: {str(e)}\"\n",
      "\n",
      "                if generate_analysis:\n",
      "                    try:\n",
      "                        logging.info(\"Generating comprehensive analysis\")\n",
      "                        combined_analysis = generate_issue_analysis_output(\n",
      "                            articles_sorted=articles_sorted,\n",
      "                            company_name=company_name,\n",
      "                            general_folder=general_folder,\n",
      "                            industry_of_interest=industry_of_interest,\n",
      "                            region=region,\n",
      "                            language=language\n",
      "                        )\n",
      "                    except Exception as e:\n",
      "                        logging.error(f\"Error generating analysis: {str(e)}\")\n",
      "                        combined_analysis = f\"Error generating analysis: {str(e)}\"\n",
      "\n",
      "                if generate_topics:\n",
      "                    try:\n",
      "                        logging.info(\"Generating topic summaries\")\n",
      "                        topics_summaries = generate_topics_output(\n",
      "                            articles_sorted=articles_sorted,\n",
      "                            company_name=company_name,\n",
      "                            general_folder=general_folder,\n",
      "                            industry_of_interest=industry_of_interest,\n",
      "                            region=region,\n",
      "                            language=language\n",
      "                        )\n",
      "                    except Exception as e:\n",
      "                        logging.error(f\"Error generating topics: {str(e)}\")\n",
      "                        topics_summaries = f\"Error generating topics: {str(e)}\"\n",
      "\n",
      "                if generate_analytics:\n",
      "                    try:\n",
      "                        logging.info(\"Generating media analytics\")\n",
      "                        media_analytics = generate_analytics_output(\n",
      "                            articles_sorted=articles_sorted,\n",
      "                            company_name=company_name,\n",
      "                            general_folder=general_folder,\n",
      "                            industry_of_interest=industry_of_interest,\n",
      "                            region=region,\n",
      "                            language=language\n",
      "                        )\n",
      "                    except Exception as e:\n",
      "                        logging.error(f\"Error generating analytics: {str(e)}\")\n",
      "                        media_analytics = f\"Error generating analytics: {str(e)}\"\n",
      "\n",
      "                if generate_quotes:\n",
      "                    try:\n",
      "                        logging.info(\"Generating stakeholder analysis\")\n",
      "                        stakeholder_quotes = generate_stakeholder_quotes(\n",
      "                            articles_sorted=articles_sorted,\n",
      "                            company_name=company_name,\n",
      "                            general_folder=general_folder,\n",
      "                            language=language\n",
      "                        )\n",
      "                    except Exception as e:\n",
      "                        logging.error(f\"Error generating stakeholder quotes: {str(e)}\")\n",
      "                        stakeholder_quotes = f\"Error generating stakeholder quotes: {str(e)}\"\n",
      "\n",
      "                if generate_consolidated_quotes:\n",
      "                    try:\n",
      "                        logging.info(\"Generating consolidated stakeholder analysis\")\n",
      "                        consolidated_quotes = generate_consolidated_stakeholder_analysis(\n",
      "                            company_name=company_name,\n",
      "                            articles=articles_sorted,\n",
      "                            general_folder=general_folder,\n",
      "                            language=language\n",
      "                        )\n",
      "                    except Exception as e:\n",
      "                        logging.error(f\"Error generating consolidated quotes: {str(e)}\")\n",
      "                        consolidated_quotes = f\"Error generating consolidated quotes: {str(e)}\"\n",
      "\n",
      "                # Prepare results for company analysis\n",
      "                results = [\n",
      "                    md_content,\n",
      "                    summary_insights,\n",
      "                    combined_analysis,\n",
      "                    topics_summaries,\n",
      "                    media_analytics,\n",
      "                    stakeholder_quotes,\n",
      "                    consolidated_quotes,\n",
      "                    journalist_profile,\n",
      "                    topic_analysis_result,\n",
      "                    journalist_articles_list,\n",
      "                    journalist_analysis_result  # NEW output (not applicable)\n",
      "                ]\n",
      "                \n",
      "                # Before returning, generate the program summary\n",
      "                try:\n",
      "                    summary_path = tracker.generate_report()\n",
      "                    logging.info(f\"Program summary report generated: {summary_path}\")\n",
      "                    \n",
      "                    # Save raw data for possible future analysis\n",
      "                    tracker.save_raw_data()\n",
      "                    \n",
      "                    # Add summary information to the journalist_analysis_result field\n",
      "                    if isinstance(results[-1], str):\n",
      "                        report_name = os.path.basename(summary_path)\n",
      "                        results[-1] += f\"\\n\\n---\\n\\n## Program Analysis Summary\\n\\nA detailed execution report has been generated: {report_name}\\n\\n\"\n",
      "                        results[-1] += f\"* **Total AI Model Calls:** {len(tracker.chatbot_calls)}\\n\"\n",
      "                        results[-1] += f\"* **Articles Processed:** {tracker.articles_processed}\\n\" \n",
      "                        results[-1] += f\"* **Total Execution Time:** {(time.time() - tracker.start_time)/60:.2f} minutes\\n\"\n",
      "                except Exception as e:\n",
      "                    logging.error(f\"Error generating program summary: {str(e)}\")\n",
      "                \n",
      "                return results\n",
      "\n",
      "            except Exception as e:\n",
      "                error_msg = f\"Error in company analysis: {str(e)}\"\n",
      "                logging.error(error_msg)\n",
      "                logging.error(traceback.format_exc())\n",
      "                return [error_msg] * 11\n",
      "\n",
      "    except Exception as e:\n",
      "        error_msg = f\"An error occurred: {str(e)}\\nPlease check the app.log file for more details.\"\n",
      "        logging.error(f\"Error in Gradio interface: {str(e)}\")\n",
      "        logging.error(traceback.format_exc())\n",
      "        return [error_msg] * 11\n",
      "\n",
      "def update_interface(analysis_type):\n",
      "    if analysis_type == \"Media Coverage Analysis\":\n",
      "        return {\n",
      "            company_name: gr.update(visible=True),\n",
      "            industry: gr.update(visible=True),\n",
      "            region: gr.update(visible=True),\n",
      "            journalist_name: gr.update(visible=False),\n",
      "            topic_focus: gr.update(visible=False),\n",
      "            generate_journalist_article_list: gr.update(visible=False),  # Fixed name here\n",
      "            generate_journalist_coverage_profile: gr.update(visible=False),\n",
      "            generate_topic_analysis: gr.update(visible=False),\n",
      "            checkbox_container_media: gr.update(visible=True),\n",
      "            checkbox_container_journalist: gr.update(visible=False),\n",
      "            output_container_media: gr.update(visible=True),\n",
      "            journalist_articles_list_output: gr.update(visible=False),\n",
      "            output_container_journalist: gr.update(visible=False)\n",
      "        }\n",
      "    else:  # Journalist Analysis\n",
      "        return {\n",
      "            company_name: gr.update(visible=False),\n",
      "            industry: gr.update(visible=False),\n",
      "            region: gr.update(visible=False),\n",
      "            journalist_name: gr.update(visible=True),\n",
      "            topic_focus: gr.update(visible=True),\n",
      "            generate_journalist_article_list: gr.update(visible=True),  # Fixed name here\n",
      "            generate_journalist_coverage_profile: gr.update(visible=True),\n",
      "            generate_topic_analysis: gr.update(visible=True),\n",
      "            checkbox_container_media: gr.update(visible=False),\n",
      "            checkbox_container_journalist: gr.update(visible=True),\n",
      "            output_container_media: gr.update(visible=False),\n",
      "            journalist_articles_list_output: gr.update(visible=True),\n",
      "            output_container_journalist: gr.update(visible=True)\n",
      "        }\n",
      "\n",
      "\n",
      "with gr.Blocks(title=\"Media and Journalist Analysis Tool\") as iface:\n",
      "    gr.Markdown(\"# Media and Journalist Analysis Tool\")\n",
      "    \n",
      "    # Analysis Type Selection\n",
      "    analysis_type = gr.Radio(\n",
      "        choices=[\"Media Coverage Analysis\", \"Journalist Analysis\"],\n",
      "        label=\"Select Analysis Type\",\n",
      "        value=\"Media Coverage Analysis\"\n",
      "    )\n",
      "    \n",
      "    # Common Inputs\n",
      "    with gr.Row():\n",
      "        file_folder = gr.Textbox(label=\"PDF File Folder Path (Optional)\", placeholder=\"Enter path to PDF folder\")\n",
      "        docx_file = gr.Textbox(label=\"DOCX File Path (Optional)\", placeholder=\"Enter path to DOCX file\")\n",
      "    \n",
      "    language = gr.Dropdown(\n",
      "        label=\"Output Language\",\n",
      "        choices=[\"English\", \"French\", \"German\", \"Spanish\", \"Italian\", \"Dutch\"],\n",
      "        value=\"English\"\n",
      "    )\n",
      "    \n",
      "    force_reprocess = gr.Checkbox(label=\"Force Reprocess\", value=False)\n",
      "    \n",
      "    # Media Coverage Analysis Inputs\n",
      "    with gr.Group() as media_inputs:\n",
      "        company_name = gr.Textbox(label=\"Company Name\", placeholder=\"Enter company name\")\n",
      "        industry = gr.Textbox(label=\"Industry of Interest\", placeholder=\"Enter industry\")\n",
      "        region = gr.Textbox(label=\"Region\", placeholder=\"Enter region\")\n",
      "    \n",
      "    # Journalist Analysis Inputs\n",
      "    with gr.Group() as journalist_inputs:\n",
      "        journalist_name = gr.Textbox(\n",
      "            label=\"Journalist Name\",\n",
      "            placeholder=\"Enter journalist name\",\n",
      "            visible=False\n",
      "        )\n",
      "        topic_focus = gr.Textbox(\n",
      "            label=\"Topic Focus\",\n",
      "            placeholder=\"Enter specific topic to analyze (e.g., 'climate change', 'economic policy')\",\n",
      "            visible=False\n",
      "        )\n",
      "    \n",
      "    # Media Coverage Analysis Checkboxes\n",
      "    with gr.Group() as checkbox_container_media:\n",
      "        generate_journalist_list = gr.Checkbox(label=\"Generate Articles' reference list\", value=True)\n",
      "        generate_insights = gr.Checkbox(label=\"Generate Insights\", value=False)\n",
      "        generate_analysis = gr.Checkbox(label=\"Generate Comprehensive Issues Analysis\", value=False)\n",
      "        generate_topics = gr.Checkbox(label=\"Generate Topic Summaries\", value=False)\n",
      "        generate_analytics = gr.Checkbox(label=\"Generate Media Analytics\", value=False)\n",
      "        generate_quotes = gr.Checkbox(label=\"Generate Stakeholder Quotes Analysis\", value=False)\n",
      "        generate_consolidated_quotes = gr.Checkbox(label=\"Generate Consolidated Stakeholder Analysis\", value=False)\n",
      "    \n",
      "    # Journalist Analysis Options\n",
      "    with gr.Group() as checkbox_container_journalist:\n",
      "        gr.Markdown(\"### Select Analysis Type(s)\")\n",
      "        generate_journalist_article_list = gr.Checkbox(  # Define the component here\n",
      "            label=\"Generate Articles List\",\n",
      "            info=\"Generate a list of all articles written by the journalist\",\n",
      "            value=True,\n",
      "            visible=False\n",
      "        )\n",
      "        generate_journalist_coverage_profile = gr.Checkbox(\n",
      "            label=\"Generate Full Journalist Coverage Profile\",\n",
      "            info=\"Analyze the journalist's overall coverage patterns and style\",\n",
      "            value=False,\n",
      "            visible=False\n",
      "        )\n",
      "        generate_topic_analysis = gr.Checkbox(\n",
      "            label=\"Generate Topic-Focused Analysis\",\n",
      "            info=\"Analyze the journalist's coverage of a specific topic\",\n",
      "            value=False,\n",
      "            visible=False\n",
      "        )\n",
      "        generate_journalist_analysis = gr.Checkbox(\n",
      "            label=\"Generate Journalist Analytics Report\",\n",
      "            info=\"Generate a comprehensive analytics report including visualizations and AI-powered insights\",\n",
      "            value=False,\n",
      "            visible=True\n",
      "        )\n",
      "    \n",
      "    # Media Coverage Analysis Outputs\n",
      "    with gr.Group() as output_container_media:\n",
      "        journalist_list_output = gr.Textbox(label=\"Articles' reference list\")\n",
      "        insights_output = gr.Textbox(label=\"Summary Insights\")\n",
      "        analysis_output = gr.Textbox(label=\"Comprehensive Issues Analysis\")\n",
      "        topics_output = gr.Textbox(label=\"Topics Summaries\")\n",
      "        analytics_output = gr.Textbox(label=\"Media Analytics Report\")\n",
      "        stakeholder_output = gr.Textbox(label=\"Stakeholder Quotes Analysis\")\n",
      "        consolidated_stakeholder_output = gr.Textbox(label=\"Consolidated Stakeholder Analysis\")\n",
      "    \n",
      "    # Journalist Analysis Outputs\n",
      "    with gr.Group() as output_container_journalist:\n",
      "        journalist_articles_list_output = gr.Textbox(  # Add this new line\n",
      "            label=\"Journalist's Articles List\",\n",
      "            visible=False\n",
      "        )\n",
      "        journalist_profile_output = gr.Textbox(\n",
      "            label=\"Journalist Profile Analysis\",\n",
      "            visible=False\n",
      "        )\n",
      "        topic_analysis_output = gr.Textbox(\n",
      "            label=\"Topic-Focused Analysis\",\n",
      "            visible=False\n",
      "        )\n",
      "        journalist_analysis_output = gr.Textbox(\n",
      "        label=\"Journalist Analytics Report\",\n",
      "        visible=True\n",
      "    )\n",
      "    \n",
      "    # Submit Button\n",
      "    submit_btn = gr.Button(\"Generate Analysis\")\n",
      "    \n",
      "    # Connect interface update function\n",
      "    # Connect interface update function\n",
      "    analysis_type.change(\n",
      "        fn=update_interface,\n",
      "        inputs=[analysis_type],\n",
      "        outputs=[\n",
      "            company_name, industry, region, journalist_name, topic_focus,\n",
      "            generate_journalist_article_list,  # Fixed name here\n",
      "            generate_journalist_coverage_profile, generate_topic_analysis,\n",
      "            checkbox_container_media, checkbox_container_journalist,\n",
      "            output_container_media, journalist_articles_list_output,\n",
      "            output_container_journalist\n",
      "        ]\n",
      "    )\n",
      "    \n",
      "    # Connect the submit button to the main processing function\n",
      "    submit_btn.click(\n",
      "        fn=gradio_journalist_list,\n",
      "        inputs=[\n",
      "            company_name,           # UI component references\n",
      "            file_folder, \n",
      "            docx_file, \n",
      "            industry, \n",
      "            region,\n",
      "            journalist_name, \n",
      "            language, \n",
      "            force_reprocess,\n",
      "            generate_journalist_list,     # Checkbox for articles list\n",
      "            generate_insights,\n",
      "            generate_analysis, \n",
      "            generate_topics,\n",
      "            generate_analytics, \n",
      "            generate_quotes,\n",
      "            generate_consolidated_quotes, \n",
      "            generate_journalist_article_list,  \n",
      "            generate_journalist_coverage_profile,\n",
      "            generate_topic_analysis,\n",
      "            generate_journalist_analysis,   # NEW checkbox input\n",
      "            topic_focus\n",
      "        ],\n",
      "        outputs=[\n",
      "            journalist_list_output,        # UI component references\n",
      "            insights_output,\n",
      "            analysis_output, \n",
      "            topics_output,\n",
      "            analytics_output, \n",
      "            stakeholder_output,\n",
      "            consolidated_stakeholder_output, \n",
      "            journalist_profile_output,\n",
      "            topic_analysis_output, \n",
      "            journalist_articles_list_output,\n",
      "            journalist_analysis_output      # NEW output textbox\n",
      "        ]\n",
      "    )\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    iface.launch()\n",
      "```\n",
      "\n",
      "## Classes/SimplifiedChatbots.py\n",
      "\n",
      "\n",
      "### File Statistics\n",
      "- Lines of code: 329\n",
      "\n",
      "### Dependencies\n",
      "The following modules are imported:\n",
      "- `PyPDF2.PdfReader`\n",
      "- `dataclasses.dataclass`\n",
      "- `datetime.datetime`\n",
      "- `google.api_core.exceptions.ResourceExhausted`\n",
      "- `google.cloud.aiplatform`\n",
      "- `google.generativeai`\n",
      "- `json`\n",
      "- `langchain_openai.ChatOpenAI`\n",
      "- `logging`\n",
      "- `openai`\n",
      "- `openai.OpenAI`\n",
      "- `os`\n",
      "- `time`\n",
      "- `typing.Any`\n",
      "- `typing.Dict`\n",
      "- `typing.List`\n",
      "- `typing.Optional`\n",
      "- `typing.Union`\n",
      "- `uuid`\n",
      "\n",
      "### Module Description\n",
      "⚠️ No module-level docstring found. Consider adding a description of this module's purpose.\n",
      "\n",
      "### Source Code\n",
      "```python\n",
      "import json\n",
      "import os\n",
      "from datetime import datetime\n",
      "import logging\n",
      "import uuid\n",
      "import openai \n",
      "from openai import OpenAI\n",
      "import time\n",
      "from google.cloud import aiplatform\n",
      "import google.generativeai as genai\n",
      "from dataclasses import dataclass\n",
      "from typing import List, Optional, Union, Dict, Any\n",
      "from google.api_core.exceptions import ResourceExhausted as RateLimitError\n",
      "from langchain_openai import ChatOpenAI\n",
      "from PyPDF2 import PdfReader\n",
      "\n",
      "# New classes to use more llm\n",
      "\n",
      "@dataclass\n",
      "class Message:\n",
      "    role: str\n",
      "    content: str\n",
      "    timestamp: str\n",
      "\n",
      "class ModelConfig:\n",
      "    \"\"\"Configuration class for different model providers\"\"\"\n",
      "    \n",
      "    OPENAI_MODELS = {\n",
      "        \"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-turbo-preview\",\n",
      "        \"gpt-4o-mini\", \"chatgpt-4o-latest\", \"o3-mini-2025-01-31\", \"o3-mini\"\n",
      "    }\n",
      "    \n",
      "    GEMINI_MODELS = {\n",
      "        \"gemini-pro\", \"gemini-pro-vision\", \"gemini-1.5-pro\", \"gemini-2.0-flash-exp\", \"models/gemini-1.5-pro\", \"gemini-1.5-pro-latest\"\n",
      "    }\n",
      "    \n",
      "    @staticmethod\n",
      "    def is_openai_model(model_name: str) -> bool:\n",
      "        return model_name in ModelConfig.OPENAI_MODELS\n",
      "    \n",
      "    @staticmethod\n",
      "    def is_gemini_model(model_name: str) -> bool:\n",
      "        return model_name in ModelConfig.GEMINI_MODELS\n",
      "\n",
      "class ChatClient:\n",
      "    \"\"\"Abstract base class for chat clients\"\"\"\n",
      "    \n",
      "    def invoke(self, messages: List[Dict[str, str]]) -> str:\n",
      "        raise NotImplementedError\n",
      "\n",
      "class OpenAIChatClient(ChatClient):\n",
      "    def __init__(self, model_name: str, max_tokens: int, temperature: float):\n",
      "        from openai import OpenAI\n",
      "        self.client = OpenAI()  \n",
      "        self.model_name = model_name\n",
      "        self.max_tokens = max_tokens\n",
      "        self.temperature = temperature\n",
      "        \n",
      "    def invoke(self, messages: List[Dict[str, str]]) -> str:\n",
      "        # For models that require max_completion_tokens and do not support temperature.\n",
      "        if self.model_name in {\"o3-mini\", \"o3-mini-2025-01-31\"}:\n",
      "            response = self.client.chat.completions.create(\n",
      "                model=self.model_name,\n",
      "                messages=messages,\n",
      "                max_completion_tokens=self.max_tokens  # Use alternative parameter name.\n",
      "            )\n",
      "        else:\n",
      "            response = self.client.chat.completions.create(\n",
      "                model=self.model_name,\n",
      "                messages=messages,\n",
      "                max_tokens=self.max_tokens,\n",
      "                temperature=self.temperature\n",
      "            )\n",
      "        return response.choices[0].message.content\n",
      "\n",
      "\n",
      "class GeminiChatClient(ChatClient):\n",
      "    def __init__(self, model_name: str, max_tokens: int, temperature: float):\n",
      "        # Initialize Gemini client\n",
      "        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
      "        self.model = genai.GenerativeModel(model_name)\n",
      "        self.max_tokens = max_tokens\n",
      "        self.temperature = temperature\n",
      "        \n",
      "    def invoke(self, messages: List[Dict[str, str]]) -> str:\n",
      "        # Convert messages to Gemini format\n",
      "        chat = self.model.start_chat()\n",
      "        \n",
      "        # Process each message\n",
      "        for message in messages:\n",
      "            if message[\"role\"] == \"system\":\n",
      "                # Add system message as user message with special prefix\n",
      "                chat.send_message(f\"[System Instructions]: {message['content']}\")\n",
      "            elif message[\"role\"] == \"user\":\n",
      "                chat.send_message(message[\"content\"])\n",
      "            elif message[\"role\"] == \"assistant\":\n",
      "                # Skip assistant messages as they're responses\n",
      "                continue\n",
      "        \n",
      "        # Generate response with configured parameters\n",
      "        response = chat.send_message(\n",
      "            messages[-1][\"content\"],\n",
      "            generation_config=genai.types.GenerationConfig(\n",
      "                max_output_tokens=self.max_tokens,\n",
      "                temperature=self.temperature\n",
      "            )\n",
      "        )\n",
      "        \n",
      "        return response.text\n",
      "\n",
      "# Base class for common chatbot functionalities\n",
      "class Chatbot:\n",
      "    def __init__(self, \n",
      "                 json_save_path=None, \n",
      "                 docs_paths=None, \n",
      "                 model_name=\"gpt-4o-mini\", \n",
      "                 max_tokens=750, \n",
      "                 temperature=0):\n",
      "        \"\"\"\n",
      "        Initializes the chatbot\n",
      "        \"\"\"\n",
      "\n",
      "        self.json_save_path = json_save_path\n",
      "        self.initialize_chat_dict(json_save_path, docs_paths, model_name, max_tokens, temperature)\n",
      "        \n",
      "        # New: Initialize appropriate client based on model name\n",
      "        if ModelConfig.is_openai_model(model_name):\n",
      "            self.chat_client = OpenAIChatClient(model_name, max_tokens, temperature)\n",
      "        elif ModelConfig.is_gemini_model(model_name):\n",
      "            self.chat_client = GeminiChatClient(model_name, max_tokens, temperature)\n",
      "        else:\n",
      "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
      "\n",
      "    def initialize_chat_dict(self, json_save_path, docs_paths, model_name, max_tokens, temperature):\n",
      "        \"\"\"New: Separated initialization logic for better organization\"\"\"\n",
      "        if json_save_path:\n",
      "            try:\n",
      "                with open(json_save_path, 'r') as file:\n",
      "                    self.chat_dict = json.load(file)\n",
      "                self.docs_paths = self.chat_dict[\"docs_paths\"]\n",
      "                self.model_name = self.chat_dict[\"model_name\"]\n",
      "                self.max_tokens = self.chat_dict[\"max_tokens\"]\n",
      "                self.temperature = self.chat_dict[\"temperature\"]\n",
      "            except Exception as e:\n",
      "                print(f\"Error loading chat from {json_save_path}: {e}\")\n",
      "        else:\n",
      "            try:\n",
      "                self.chat_dict = {\n",
      "                    \"id\": datetime.now().strftime(\"%d:%m:%y_%H.%M.%S\"),\n",
      "                    \"docs_paths\": docs_paths,\n",
      "                    \"model_name\": model_name,\n",
      "                    \"max_tokens\": max_tokens,\n",
      "                    \"temperature\": temperature,\n",
      "                    \"messages\": [],\n",
      "                }\n",
      "                self.docs_paths = docs_paths\n",
      "                self.model_name = model_name\n",
      "                self.max_tokens = max_tokens\n",
      "                self.temperature = temperature\n",
      "            except Exception as e:\n",
      "                print(f\"Error initializing chat: {e}\")\n",
      "        \n",
      "        if self.json_save_path:\n",
      "            try: \n",
      "                with open(self.json_save_path, 'r') as file:\n",
      "                    self.chat_dict = json.load(file)\n",
      "\n",
      "                self.docs_paths = self.chat_dict[\"docs_paths\"]\n",
      "                self.model_name = self.chat_dict[\"model_name\"]\n",
      "                self.max_tokens = self.chat_dict[\"max_tokens\"]\n",
      "                self.temperature = self.chat_dict[\"temperature\"]\n",
      "            except Exception as e:\n",
      "                print(f\"Error loading chat from {self.json_save_path}: {e}\")\n",
      "        else:\n",
      "            try:\n",
      "                self.chat_dict = {\n",
      "                    \"id\": datetime.now().strftime(\"%d:%m:%y_%H.%M.%S\"),\n",
      "                    \"docs_paths\": docs_paths,\n",
      "                    \"model_name\": model_name,\n",
      "                    \"max_tokens\": max_tokens,\n",
      "                    \"temperature\": temperature,\n",
      "                    \"messages\": [],\n",
      "                }\n",
      "\n",
      "                self.docs_paths = docs_paths\n",
      "                self.model_name = model_name\n",
      "                self.max_tokens = max_tokens\n",
      "                self.temperature = temperature\n",
      "            except Exception as e:\n",
      "                print(f\"Error initializing chat: {e}\")\n",
      "\n",
      "    def add_message(self, message, role=\"user\"):\n",
      "        \"\"\"\n",
      "        Adds a message to the chat dictionary.\n",
      "        \"\"\"\n",
      "        self.chat_dict[\"messages\"].append({\n",
      "            \"role\": role,\n",
      "            \"content\": message,\n",
      "            \"timestamp\": datetime.now().isoformat()\n",
      "        })\n",
      "\n",
      "    def ask(self, question):\n",
      "        raise NotImplementedError(\"This method should be implemented by subclasses.\")\n",
      "\n",
      "    def save_chat_to_json(self, folder_path):\n",
      "        \"\"\"\n",
      "        Saves the chat to a JSON file.\n",
      "        \"\"\"\n",
      "        filename = f\"chat_{self.chat_dict['id']}.json\"\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "        try:\n",
      "            with open(file_path, 'w') as file:\n",
      "                json.dump(self.chat_dict, file, indent=4)\n",
      "        except Exception as e:\n",
      "            print(f\"Error saving chat to {file_path}: {e}\")\n",
      "\n",
      "    def save_chat_to_md(self, folder_path):\n",
      "        \"\"\"\n",
      "        Saves the chat to a markdown file.\n",
      "        \"\"\"\n",
      "        filename = f\"chat_{self.chat_dict['id']}.md\"\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "        try:\n",
      "            with open(file_path, 'w') as file:\n",
      "                for message in self.chat_dict[\"messages\"]:\n",
      "                    file.write(f\"## {message['role'].capitalize()}\\n\\n{message['content']}\\n\\n\")\n",
      "        except Exception as e:\n",
      "            print(f\"Error saving chat to {file_path}: {e}\")\n",
      "\n",
      "class ChatGPT(Chatbot):\n",
      "    def __init__(self, \n",
      "                 system_prompt=\"You are a helpful assistant!\",\n",
      "                 max_retries=3,\n",
      "                 retry_delay=2,\n",
      "                 **kwargs):\n",
      "        \"\"\"\n",
      "        Initializes the GPT chatbot.\n",
      "        \"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.max_retries = max_retries\n",
      "        self.retry_delay = retry_delay\n",
      "\n",
      "        try:\n",
      "            # For models like \"o3-mini\" that don't support temperature,\n",
      "            # use max_completion_tokens and omit temperature.\n",
      "            if self.model_name in {\"o3-mini\", \"o3-mini-2025-01-31\"}:\n",
      "                self.chat_openAI = ChatOpenAI(\n",
      "                    model_name=self.model_name,\n",
      "                    max_completion_tokens=self.max_tokens,\n",
      "                    request_timeout=90  # Increased timeout\n",
      "                )\n",
      "            else:\n",
      "                self.chat_openAI = ChatOpenAI(\n",
      "                    model_name=self.model_name,\n",
      "                    max_tokens=self.max_tokens,\n",
      "                    temperature=self.temperature,\n",
      "                    request_timeout=90,  # Increased timeout\n",
      "                )\n",
      "            self.add_message(system_prompt, role=\"system\")\n",
      "        except Exception as e:\n",
      "            print(f\"Error initializing GPT chatbot: {e}\")\n",
      "\n",
      "    def ask(self, question):\n",
      "        \"\"\"\n",
      "        Asks the chatbot a question with retry logic for rate-limit (429) errors.\n",
      "        \"\"\"\n",
      "        self.add_message(question, role=\"user\")\n",
      "        attempt = 0\n",
      "        delay = self.retry_delay\n",
      "        while attempt < self.max_retries:\n",
      "            try:\n",
      "                # Using the abstracted chat_client\n",
      "                ai_response = self.chat_client.invoke(self.chat_dict[\"messages\"])\n",
      "                if ai_response:\n",
      "                    self.add_message(ai_response, role=\"assistant\")\n",
      "                    # Optionally save chat history\n",
      "                    try:\n",
      "                        self.save_chat_to_json(\"Chats_Database/JSON\")\n",
      "                        self.save_chat_to_md(\"Chats_Database/MD\")\n",
      "                    except Exception as save_error:\n",
      "                        print(f\"Warning: Could not save chat history: {save_error}\")\n",
      "                    return ai_response\n",
      "            except RateLimitError as e:\n",
      "                print(f\"Rate limit reached on attempt {attempt + 1}: {e}. Waiting {delay} seconds.\")\n",
      "                time.sleep(delay)\n",
      "                delay *= 2  # Exponential backoff\n",
      "                attempt += 1\n",
      "            except Exception as e:\n",
      "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
      "                time.sleep(delay)\n",
      "                delay *= 2\n",
      "                attempt += 1\n",
      "\n",
      "        raise Exception(f\"Failed after {self.max_retries} attempts due to rate limit or other errors.\")\n",
      "\n",
      "class BigSummarizerGPT(Chatbot):\n",
      "    def __init__(self, \n",
      "                 system_prompt=\"You are a helpful assistant!\",\n",
      "                 **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.add_message(system_prompt, role=\"system\")\n",
      "\n",
      "    def get_document_loader(self, document_path: str) -> str:\n",
      "        \"\"\"\n",
      "        Loads the content of a document based on its file type.\n",
      "        \n",
      "        Args:\n",
      "            document_path (str): Path to the document\n",
      "            \n",
      "        Returns:\n",
      "            str: The content of the document as a string.\n",
      "        \"\"\"\n",
      "        document_path = document_path.strip()\n",
      "        \n",
      "        if document_path.endswith('.pdf'):\n",
      "            try:\n",
      "                reader = PdfReader(document_path)\n",
      "                text = \"\"\n",
      "                for page in reader.pages:\n",
      "                    text += page.extract_text() or \"\"\n",
      "                return text.strip()\n",
      "            except Exception as e:\n",
      "                raise ValueError(f\"Failed to load PDF file: {str(e)}\")\n",
      "        \n",
      "        elif document_path.endswith(('.txt', '.md')):\n",
      "            try:\n",
      "                with open(document_path, 'r', encoding='utf-8') as file:\n",
      "                    return file.read().strip()\n",
      "            except Exception as e:\n",
      "                raise ValueError(f\"Failed to load text/markdown file: {str(e)}\")\n",
      "        \n",
      "        else:\n",
      "            raise ValueError(f\"Unsupported file type for document: {document_path}\")\n",
      "\n",
      "    def split_text(self, text, max_chunk_size=20500):\n",
      "        chunks = []\n",
      "        current_chunk = \"\"\n",
      "        for sentence in text.split(\".\"):\n",
      "            if len(current_chunk) + len(sentence) < max_chunk_size:\n",
      "                current_chunk += sentence + \".\"\n",
      "            else:\n",
      "                chunks.append(current_chunk.strip())\n",
      "                current_chunk = sentence + \".\"\n",
      "        if current_chunk:\n",
      "            chunks.append(current_chunk.strip())\n",
      "        return chunks\n",
      "\n",
      "    def crop_dict(self, chat_bot_dict: Dict, max_token: int) -> Dict:\n",
      "        \"\"\"\n",
      "        Trims the chat history to ensure the total token count stays within the limit.\n",
      "        \n",
      "        Args:\n",
      "            chat_bot_dict (dict): The chat dictionary containing messages.\n",
      "            max_token (int): The maximum allowable token count.\n",
      "        \n",
      "        Returns:\n",
      "            dict: The trimmed chat dictionary.\n",
      "        \"\"\"\n",
      "        total_character = 0\n",
      "        chat_bot_dict[\"messages\"].reverse()  # Start removing from the oldest messages\n",
      "        for message in chat_bot_dict[\"messages\"][:-1]:  # Exclude the last message (current query)\n",
      "            total_character += len(message[\"content\"])\n",
      "            # Divide by 4 to estimate tokens from characters (approximation)\n",
      "            if total_character / 4 > max_token:\n",
      "                chat_bot_dict[\"messages\"].remove(message)\n",
      "        chat_bot_dict[\"messages\"].reverse()  # Restore original order\n",
      "        return chat_bot_dict\n",
      "\n",
      "    def ask(self, question, document_path, max_chunk_size=20000, dict_max_token=15000, max_retries=3, retry_delay=2):\n",
      "        \"\"\"\n",
      "        Asks the chatbot a question about a document.\n",
      "\n",
      "        Args:\n",
      "            question (str): The question to ask.\n",
      "            document_path (str): Path to the document.\n",
      "            max_chunk_size (int): Maximum size of text chunks.\n",
      "            dict_max_token (int): Maximum number of tokens in the chat history.\n",
      "            max_retries (int): Maximum number of retries per chunk.\n",
      "            retry_delay (int): Initial delay (in seconds) before retrying.\n",
      "\n",
      "        Returns:\n",
      "            str: Concatenated summaries from all chunks.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            # Validate the document path\n",
      "            if not document_path or not isinstance(document_path, str):\n",
      "                raise ValueError(\"Invalid document path provided\")\n",
      "\n",
      "            if not os.path.exists(document_path):\n",
      "                raise FileNotFoundError(f\"Document not found at path: {document_path}\")\n",
      "\n",
      "            # Load document content\n",
      "            doc_content = self.get_document_loader(document_path)\n",
      "            if not doc_content.strip():\n",
      "                raise ValueError(\"Document appears to be empty\")\n",
      "\n",
      "            chunks = self.split_text(doc_content, max_chunk_size)\n",
      "            previous_summaries = []\n",
      "\n",
      "            for i, chunk in enumerate(chunks):\n",
      "                print(f\"Processing chunk: {i+1}/{len(chunks)}\")\n",
      "\n",
      "                system_prompt = f\"\"\"You are a helpful assistant!\n",
      "Previous summaries: {' '.join(previous_summaries)}\"\"\"\n",
      "\n",
      "                new_bot = BigSummarizerGPT(\n",
      "                    system_prompt=system_prompt,\n",
      "                    model_name=self.model_name,\n",
      "                    max_tokens=self.max_tokens,\n",
      "                    temperature=self.temperature\n",
      "                )\n",
      "\n",
      "                user_prompt = f\"\"\"{question}\\n\\nHere is the document section to analyze:\\n\\n{chunk}\"\"\"\n",
      "                new_bot.add_message(user_prompt, role=\"user\")\n",
      "                new_bot.crop_dict(new_bot.chat_dict, dict_max_token)\n",
      "\n",
      "                # Retry loop for this chunk\n",
      "                attempt = 0\n",
      "                delay = retry_delay\n",
      "                response = None\n",
      "                while attempt < max_retries:\n",
      "                    try:\n",
      "                        response = new_bot.chat_client.invoke(new_bot.chat_dict[\"messages\"])\n",
      "                        break  # Success: exit the retry loop\n",
      "                    except RateLimitError as e:\n",
      "                        print(f\"Attempt {attempt + 1} for chunk {i+1} failed with rate-limit error: {e}. Waiting {delay} seconds.\")\n",
      "                        time.sleep(delay)\n",
      "                        delay *= 2  # Exponential backoff\n",
      "                        attempt += 1\n",
      "                    except Exception as e:\n",
      "                        print(f\"Attempt {attempt + 1} for chunk {i+1} failed with error: {e}. Waiting {delay} seconds.\")\n",
      "                        time.sleep(delay)\n",
      "                        delay *= 2\n",
      "                        attempt += 1\n",
      "\n",
      "                if response is None:\n",
      "                    print(f\"Skipping chunk {i+1} after {max_retries} attempts due to errors.\")\n",
      "                    continue  # Skip this chunk if still failing\n",
      "                \n",
      "                new_bot.add_message(response, role=\"assistant\")\n",
      "                previous_summaries.append(response)\n",
      "\n",
      "                # Save chat history\n",
      "                try:\n",
      "                    new_bot.save_chat_to_json(\"Chats_Database/JSON\")\n",
      "                    new_bot.save_chat_to_md(\"Chats_Database/MD\")\n",
      "                except Exception as save_error:\n",
      "                    print(f\"Warning: Could not save chat history: {save_error}\")\n",
      "\n",
      "            return \"\\n\\n\".join(previous_summaries)\n",
      "\n",
      "\n",
      "        except Exception as e:\n",
      "            print(f\"Unexpected error: {str(e)}\")\n",
      "            raise\n",
      "\n",
      "```\n",
      "\n",
      "## Classes/DocumentProcessor.py\n",
      "\n",
      "\n",
      "### File Statistics\n",
      "- Lines of code: 230\n",
      "\n",
      "### Dependencies\n",
      "The following modules are imported:\n",
      "- `PyPDF2.PdfReader`\n",
      "- `collections.OrderedDict`\n",
      "- `dataclasses.dataclass`\n",
      "- `datetime.datetime`\n",
      "- `dateutil.parser`\n",
      "- `difflib.SequenceMatcher`\n",
      "- `docx.Document`\n",
      "- `logging`\n",
      "- `os`\n",
      "- `pandas`\n",
      "- `re`\n",
      "- `typing.Dict`\n",
      "- `typing.List`\n",
      "- `typing.Optional`\n",
      "- `typing.Tuple`\n",
      "- `unicodedata`\n",
      "\n",
      "### Module Description\n",
      "⚠️ No module-level docstring found. Consider adding a description of this module's purpose.\n",
      "\n",
      "### Source Code\n",
      "```python\n",
      "import os\n",
      "import pandas as pd\n",
      "from difflib import SequenceMatcher\n",
      "from PyPDF2 import PdfReader\n",
      "from docx import Document as DocxDocument\n",
      "from difflib import SequenceMatcher\n",
      "import logging\n",
      "from datetime import datetime\n",
      "from typing import Optional, Dict, List, Tuple\n",
      "from collections import OrderedDict\n",
      "import re\n",
      "import logging\n",
      "from dataclasses import dataclass\n",
      "from dateutil import parser\n",
      "import unicodedata\n",
      "\n",
      "# Class to process document inputs\n",
      "class DocumentProcessor:\n",
      "    def __init__(self, min_length: int = 1000, max_length: int = 25500, similarity_threshold: float = 0.8):\n",
      "        \"\"\"\n",
      "        Initialize the document processor with configurable parameters.\n",
      "        \n",
      "        Args:\n",
      "            min_length (int): Minimum character length for valid documents\n",
      "            max_length (int): Maximum character length for valid documents\n",
      "            similarity_threshold (float): Threshold for detecting duplicate content\n",
      "        \"\"\"\n",
      "        self.min_length = min_length\n",
      "        self.max_length = max_length\n",
      "        self.similarity_threshold = similarity_threshold\n",
      "        logging.basicConfig(level=logging.INFO)\n",
      "        self.logger = logging.getLogger(__name__)\n",
      "\n",
      "    def _text_similarity(self, text1: str, text2: str) -> float:\n",
      "        \"\"\"Calculate similarity ratio between two texts\"\"\"\n",
      "        return SequenceMatcher(None, text1.strip(), text2.strip()).ratio()\n",
      "\n",
      "    def _get_files(self, folder_path: str) -> List[str]:\n",
      "        \"\"\"Get list of files from folder, excluding system files\"\"\"\n",
      "        if not os.path.exists(folder_path):\n",
      "            self.logger.error(f\"Folder path does not exist: {folder_path}\")\n",
      "            return []\n",
      "        \n",
      "        files = [f\"{folder_path}/{file}\" for file in os.listdir(folder_path)]\n",
      "        return [f for f in files if f.lower().endswith('.pdf')]\n",
      "\n",
      "    def _extract_text_from_pdf(self, file_path: str) -> Optional[str]:\n",
      "        \"\"\"Extract text content from a PDF file using PyPDF2\"\"\"\n",
      "        try:\n",
      "            reader = PdfReader(file_path)\n",
      "            text = \"\"\n",
      "            for page in reader.pages:\n",
      "                text += page.extract_text() or \"\"\n",
      "            return text.strip()\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error extracting text from {file_path}: {e}\")\n",
      "            return None\n",
      "\n",
      "    def _extract_text_from_docx(self, file_path: str) -> Optional[str]:\n",
      "        \"\"\"Extract text content from a DOCX file using python-docx\"\"\"\n",
      "        try:\n",
      "            doc = DocxDocument(file_path)\n",
      "            text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
      "            return text.strip()\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error extracting text from DOCX {file_path}: {e}\")\n",
      "            return None\n",
      "\n",
      "    def process_pdf_folder(self, folder_path: str) -> List[Dict]:\n",
      "        \"\"\"Process all PDFs in a folder.\"\"\"\n",
      "        if not os.path.exists(folder_path):\n",
      "            self.logger.error(f\"PDF folder path does not exist: {folder_path}\")\n",
      "            return []\n",
      "\n",
      "        files = self._get_files(folder_path)\n",
      "        self.logger.info(f\"Found {len(files)} files in folder\")\n",
      "\n",
      "        articles = [{'file_path': file, 'position': idx + 1} for idx, file in enumerate(files)]\n",
      "\n",
      "        # Process files using PyPDF2\n",
      "        processed_documents = []\n",
      "        for file in files:\n",
      "            try:\n",
      "                pdf_text = self._extract_text_from_pdf(file)\n",
      "\n",
      "                if pdf_text is None or not self.min_length <= len(pdf_text) <= self.max_length:\n",
      "                    self.logger.info(\n",
      "                        f\"Removing file {file} (text extraction failed or length out of range)\"\n",
      "                    )\n",
      "                    continue\n",
      "\n",
      "                pdf_text = pdf_text.replace(\"William Masquelier\", \" \")  # Optional cleanup\n",
      "                processed_documents.append({'file_path': file, 'content': pdf_text})\n",
      "            except Exception as e:\n",
      "                self.logger.error(f\"Error processing PDF {file}: {e}\")\n",
      "\n",
      "        self.logger.info(f\"Processed {len(processed_documents)} valid PDF files\")\n",
      "        return processed_documents\n",
      "\n",
      "    def process_docx(self, docx_path: str, separator: str = \"--\") -> List[Dict]:\n",
      "        \"\"\"\n",
      "        Process a DOCX file containing multiple articles.\n",
      "        \n",
      "        Args:\n",
      "            docx_path (str): Path to the DOCX file\n",
      "            separator (str): Separator used between articles\n",
      "            \n",
      "        Returns:\n",
      "            List[Dict]: List of processed articles\n",
      "        \"\"\"\n",
      "        if not os.path.exists(docx_path):\n",
      "            self.logger.error(f\"DOCX file does not exist: {docx_path}\")\n",
      "            return []\n",
      "\n",
      "        try:\n",
      "            # Extract text from DOCX\n",
      "            docx_text = self._extract_text_from_docx(docx_path)\n",
      "            if not docx_text:\n",
      "                self.logger.error(f\"Failed to extract text from DOCX {docx_path}\")\n",
      "                return []\n",
      "\n",
      "            # Split into articles based on separator\n",
      "            raw_sections = docx_text.split(separator)\n",
      "            articles = []\n",
      "\n",
      "            for idx, section in enumerate(raw_sections, 1):\n",
      "                content = section.strip()\n",
      "                if not self.min_length <= len(content) <= self.max_length:\n",
      "                    continue\n",
      "\n",
      "                is_duplicate = any(\n",
      "                    self._text_similarity(content, existing['content']) > self.similarity_threshold \n",
      "                    for existing in articles\n",
      "                )\n",
      "\n",
      "                if not is_duplicate:\n",
      "                    articles.append({\n",
      "                        'file_path': docx_path,\n",
      "                        'content': content,\n",
      "                        'position': idx\n",
      "                    })\n",
      "\n",
      "            self.logger.info(f\"Processed {len(articles)} articles from DOCX\")\n",
      "            return articles\n",
      "\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error processing DOCX {docx_path}: {e}\")\n",
      "            return []\n",
      "\n",
      "    def process_documents(self, \n",
      "                         pdf_folder_path: Optional[str] = None, \n",
      "                         docx_file_path: Optional[str] = None, \n",
      "                         docx_separator: str = \"--\") -> List[Dict]:\n",
      "        \"\"\"\n",
      "        Main processing function that handles both PDF folder and DOCX file inputs.\n",
      "        \n",
      "        Args:\n",
      "            pdf_folder_path (str, optional): Path to folder containing PDFs\n",
      "            docx_file_path (str, optional): Path to DOCX file\n",
      "            docx_separator (str): Separator for DOCX processing\n",
      "            \n",
      "        Returns:\n",
      "            List[Dict]: Combined list of processed articles from both sources\n",
      "        \"\"\"\n",
      "        all_articles = []\n",
      "        position = 1\n",
      "\n",
      "        if pdf_folder_path:\n",
      "            pdf_articles = self.process_pdf_folder(pdf_folder_path)\n",
      "            for article in pdf_articles:\n",
      "                article['reordered_position'] = position\n",
      "                position += 1\n",
      "            all_articles.extend(pdf_articles)\n",
      "\n",
      "        if docx_file_path:\n",
      "            docx_articles = self.process_docx(docx_file_path, docx_separator)\n",
      "            for article in docx_articles:\n",
      "                article['reordered_position'] = position\n",
      "                position += 1\n",
      "            all_articles.extend(docx_articles)\n",
      "\n",
      "        self.logger.info(f\"Total processed articles: {len(all_articles)}\")\n",
      "        return all_articles\n",
      "\n",
      "@dataclass\n",
      "class Citation:\n",
      "    media_outlet: str\n",
      "    author: Optional[str]\n",
      "    date: str\n",
      "    title: Optional[str] = None\n",
      "    link: Optional[str] = None\n",
      "    \n",
      "    def __eq__(self, other):\n",
      "        if not isinstance(other, Citation):\n",
      "            return False\n",
      "        return (self.media_outlet.lower() == other.media_outlet.lower() and\n",
      "                self.author == other.author and\n",
      "                self.date == other.date)\n",
      "    \n",
      "    def __hash__(self):\n",
      "        return hash((self.media_outlet.lower(), self.author, self.date))\n",
      "\n",
      "class CitationProcessor:\n",
      "    def __init__(self):\n",
      "        # Months in various languages (English, French, Spanish, Dutch, Italian, German)\n",
      "        self.months = {\n",
      "            'january|janvier|enero|januari|gennaio|januar',\n",
      "            'february|février|febrero|februari|febbraio|februar',\n",
      "            'march|mars|marzo|maart|marzo|märz',\n",
      "            'april|avril|abril|april|aprile|april',\n",
      "            'may|mai|mayo|mei|maggio|mai',\n",
      "            'june|juin|junio|juni|giugno|juni',\n",
      "            'july|juillet|julio|juli|luglio|juli',\n",
      "            'august|août|agosto|augustus|agosto|august',\n",
      "            'september|septembre|septiembre|september|settembre|september',\n",
      "            'october|octobre|octubre|oktober|ottobre|oktober',\n",
      "            'november|novembre|noviembre|november|novembre|november',\n",
      "            'december|décembre|diciembre|december|dicembre|dezember'\n",
      "        }\n",
      "        self.month_pattern = '|'.join(self.months)\n",
      "        \n",
      "    def normalize_text(self, text: str) -> str:\n",
      "        \"\"\"Normalize text by removing accents and converting to lowercase.\"\"\"\n",
      "        return unicodedata.normalize('NFKD', text.lower()).encode('ASCII', 'ignore').decode('utf-8')\n",
      "\n",
      "    def is_valid_date(self, text: str) -> bool:\n",
      "        \"\"\"Check if text contains a valid date format in multiple languages.\"\"\"\n",
      "        # Normalize text for matching\n",
      "        norm_text = self.normalize_text(text)\n",
      "        \n",
      "        # Check for month names in various languages\n",
      "        if any(re.search(rf'\\b{month}\\b', norm_text) for month in self.month_pattern.split('|')):\n",
      "            return True\n",
      "            \n",
      "        # Try parsing with dateutil as fallback\n",
      "        try:\n",
      "            parser.parse(text)\n",
      "            return True\n",
      "        except:\n",
      "            return False\n",
      "\n",
      "    def is_valid_citation(self, text: str) -> bool:\n",
      "        \"\"\"\n",
      "        Determine if bracketed text is a valid citation.\n",
      "        \"\"\"\n",
      "        # Common exclusion patterns\n",
      "        if any(pattern in text for pattern in ['![', '[#', 'http', '.md', '.pdf']):\n",
      "            return False\n",
      "            \n",
      "        # Split parts and check format\n",
      "        parts = [p.strip() for p in text.split(',')]\n",
      "        \n",
      "        # Must have at least media outlet and date\n",
      "        if len(parts) < 2:\n",
      "            return False\n",
      "            \n",
      "        # Last part should be a date\n",
      "        if not self.is_valid_date(parts[-1]):\n",
      "            return False\n",
      "            \n",
      "        return True\n",
      "\n",
      "    def parse_citation(self, citation_text: str) -> Optional[Citation]:\n",
      "        \"\"\"\n",
      "        Parse citation text into structured Citation object.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            parts = [p.strip() for p in citation_text.split(',')]\n",
      "            \n",
      "            if len(parts) == 2:  # [Media outlet, date]\n",
      "                return Citation(\n",
      "                    media_outlet=parts[0],\n",
      "                    author=None,\n",
      "                    date=parts[1]\n",
      "                )\n",
      "            elif len(parts) >= 3:  # [Media outlet, Author, date]\n",
      "                return Citation(\n",
      "                    media_outlet=parts[0],\n",
      "                    author=parts[1],\n",
      "                    date=parts[2]\n",
      "                )\n",
      "            return None\n",
      "        except Exception as e:\n",
      "            logging.debug(f\"Error parsing citation: {str(e)}\")\n",
      "            return None\n",
      "\n",
      "    def process_citations_in_markdown(self, markdown_content: str) -> str:\n",
      "        \"\"\"\n",
      "        Process citations in markdown content and generate bibliography.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            references = OrderedDict()\n",
      "            \n",
      "            def process_citation_match(match):\n",
      "                full_match = match.group(0)\n",
      "                citation_text = full_match.strip('[]')\n",
      "                \n",
      "                # Check if this is a valid citation\n",
      "                if not self.is_valid_citation(citation_text):\n",
      "                    return full_match\n",
      "                    \n",
      "                # Handle multiple citations separated by semicolon\n",
      "                citations = [c.strip() for c in citation_text.split(';')]\n",
      "                ref_numbers = []\n",
      "                \n",
      "                for citation in citations:\n",
      "                    parsed_citation = self.parse_citation(citation)\n",
      "                    if parsed_citation:\n",
      "                        if citation not in references:\n",
      "                            references[citation] = len(references) + 1\n",
      "                        ref_numbers.append(str(references[citation]))\n",
      "                \n",
      "                if not ref_numbers:\n",
      "                    return full_match\n",
      "                    \n",
      "                # Create reference string\n",
      "                ref_string = ','.join(ref_numbers)\n",
      "                return f'<sup style=\"line-height: 0; vertical-align: super; font-size: smaller;\">' \\\n",
      "                       f'<a href=\"#ref-{ref_string}\" style=\"text-decoration: none;\">[{ref_string}]</a></sup>'\n",
      "            \n",
      "            # Process citations\n",
      "            citation_pattern = r'\\[[^\\]]+?\\]'\n",
      "            processed_content = re.sub(citation_pattern, process_citation_match, markdown_content)\n",
      "            \n",
      "            # Add references section\n",
      "            if references:\n",
      "                processed_content += \"\\n\\n---\\n\\n## References\\n\\n\"\n",
      "                for citation, ref_num in references.items():\n",
      "                    processed_content += f'<div id=\"ref-{ref_num}\" style=\"margin-bottom: 0.5em;\">' \\\n",
      "                                      f'{ref_num}. {citation}</div>\\n'\n",
      "            \n",
      "            return processed_content\n",
      "            \n",
      "        except Exception as e:\n",
      "            logging.error(f\"Error processing citations: {str(e)}\")\n",
      "            return markdown_content\n",
      "\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Configuration flags for file inclusion\n",
    "include_config = {\n",
    "    'main_code': True,           # MediaCoverageAnalysis.py\n",
    "    'helpers': True,              # Utils/Helpers.py  - Enabled to extract helper functions\n",
    "    'outputs': False,              # Utils/Outputs.py  - Enabled to extract output functions\n",
    "    'chatbots': True,            # Classes/SimplifiedChatbots.py\n",
    "    'doc_processor': True       # Classes/DocumentProcessor.py\n",
    "}\n",
    "\n",
    "# Output functions selection (set to True for the function you want to analyze)\n",
    "output_functions_config = {\n",
    "    'generate_journalist_list_output': False,     # Generate list of journalists and media outlets\n",
    "    'generate_insights_output': False,            # Generate article insights\n",
    "    'generate_issue_analysis_output': False,      # Generate issue analysis\n",
    "    'generate_topics_output': False,              # Generate topic summaries\n",
    "    'generate_analytics_output': False,            # Generate media analytics\n",
    "    'generate_stakeholder_quotes': False,         # Generate stakeholder analysis\n",
    "    'generate_consolidated_stakeholder_analysis': False,  # Generate consolidated stakeholder analysis\n",
    "    'generate_journalist_article_list': False,    # Generate article list for a journalist\n",
    "    'generate_journalist_profile': True,         # Generate journalist profile\n",
    "    'analyze_journalist_topic_coverage': False,   # Generate journalist topic coverage analysis\n",
    "    'generate_journalist_analysis_output': False  # Generate journalist analytics\n",
    "}\n",
    "\n",
    "# Generate and display the description\n",
    "description = generate_code_description()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
